#+TITLE: Effigy, an experiment writing a compiler

I've always been fascinated with programming languages.  It's
incredible to me that some programs take the text representation of
our ideas and turn them into code that can be executed by the
computer.

Wanting to build my own programming languages, I started to write and
rewrite [[https://github.com/clarete/wheelbarrow/blob/master/lispinho/js/main.js][lots]] [[https://github.com/clarete/yal][of]] [[https://gist.github.com/clarete/03e825a70c4b4047468cc9d07ec47e4b][little]] [[https://github.com/clarete/wheelbarrow/blob/master/lispinho/js/main2.js][lisp]] interpreters. That is a common path for
beginners.  Making these toys gave me better understanding of how
programming languages work, but also made me a better software
engineer in general.

Today I want to share an exercise of designing [[https://github.com/clarete/effigy][Effigy]], a little toy
compiler that targets the Python Virtual Machine.  It's far away from
being a production level tool, but it's a bit fancier than the Lisp
interpreters I mentioned above.

#+BEGIN_note
*Note*: Even though Python is said to be a scripting language, it does
have a [[https://github.com/python/cpython/blob/master/Python/ceval.c][virtual machine]] that executes the /bytecode/ generated by its
[[https://github.com/python/cpython/blob/master/Python/compile.c][compiler]].
#+END_note

Effigy takes a syntax that I made up and generates /bytecode/ for that
Virtual Machine.  Without further ado, let's see how it works!

* A 38k feet view of the process

  The text representation of the program is parsed into a tree
  structure that contains the meaning described in the text program.
  That tree is traversed and the /bytecode/ is emitted.

  #+BEGIN_centralized
  [[./effigy-an-experiment-writing-a-compiler-overview.png]]
  #+END_centralized

  Because of its simplicity, Effigy doesn't really apply any
  optimization to either the parse tree or to the /bytecode/.  That's
  what keeps Effigy in the toy category.  At least it will make it
  easier to understand how it works :)

  Both the initial phase, that converts text into a parse tree, and
  the second phase, that traverse the parse tree and emit code, are
  based on [[https://bford.info/pub/lang/peg.pdf][Parsing Expression Grammars]] (or PEGs for short).  I learned
  how to implement the tree scanning used in the second transformation
  from the article [[http://www.lua.inf.puc-rio.br/publications/mascarenhas11parsing.pdf][Parsing Expression Grammars for Structured Data]].
  And during the time that I was implementing Effigy, I also found the
  article [[http://www.vpri.org/pdf/tr2010003_PEG.pdf][PEG-based transformer provides front-, middle and back-end
  stages in a simple compiler]] that made me feel more comfortable with
  my choice and made me tempted to re-write the assembler as a PEG
  traversal too, but I never got to it.

  Although it's not a requirement to understand this post, I highly
  recommend reading the articles above as they were essential for me
  to really understand how PEGs could be so flexible and powerful.

* The PEG library

  Two out of the three of the main components of Effigy are built on
  top of the PEG implementation.  That makes the PEG implementation
  itself is a pretty important part of this project, so it's worth
  talking about it with some depth before moving on to the actual
  compiler.

  The main goal of the PEG library is to parse input based on a
  user-defined grammar and to capture the results [only] when parsing
  succeeds.  That happens in roughly three steps:

  1. The user-defined grammar is parsed into a grammar tree;
  2. The grammar tree and the input are fed to the grammar interpreter
     that traverses the grammar tree and try to match the input.  If
     matching is successful, it generates a parse tree with the
     captured results;
  3. The parse tree is traversed and semantic actions are applied;
     
  The first step implements a [[https://en.wikipedia.org/wiki/Recursive_descent_parser][recursive descent parser]] that is able to
  understand the syntax of PEG grammars and create a tree that is
  traversed by the second step. E.g.: Parsing the grammar following
  grammar:

  #+begin_src peg
Digit <- [0-9]+
  #+end_src

  yields the following grammar tree:

  #+begin_src effigy
  {
    'Digit': [[
      [{ name: 'oneOrMore' },
        [{ name: 'range' }, '0', '9']]
    ]]
  }
  #+end_src

  In that format, lists are considered lists unless their first item
  is an object.  In that case, they're seen as functions and the rest
  of the elements in the list are considered parameters.  That might
  resemble Lisp's [[https://en.wikipedia.org/wiki/S-expression][S-Expressions]] for some! This format is the simplest
  I found to make the grammar interpreter as simple as possible as
  well.

  In the example above, the grammar interpreter starts from the first
  rule of the grammar, ~Digit~, that triggers the function ~oneOrMore~
  with a [[https://en.wikipedia.org/wiki/Thunk][thunk]] of ~range~ with its parameters and execute it until it
  fails.

  The list of all operators available are the ones documented in
  Ford's paper with the extension for parsing lists from Medeiros'
  paper, and here they are:

  Matching Functions:
  * *Any()* - ~.~: Matches any character but fails on ~EOF~;
  * *Literal(c)* - ~"c"~: Fails if ~c~ doesn't match the character
    under the input cursor;
  * *Class(c[])* - ~[abcd]~: Fail if none of the elements of ~c[]~
    match the character under the input cursor;
  * *Range(ca, cb)* ~[a-z]~: Fail if the current character under the
    input cursor isn't between the characteres ~ca~ and ~cb~;

  Parsing Functions:
  * *ZeroOrMore(fn)* - Star Operator (*): Execute ~fn~ indefinitely
    until it fails.  All collected results are returned. It never
    fails even if it fails in the first call;
  * *OneOrMore(fn)* - Plus Operator (+): Execute ~fn~ once failing if
    this first call fails. If the first call succeeds, then prepend
    this result to the output of ~ZeroOrMore(fn)~
  * *Option(fn)* - Option Operator (?): Return the result of ~fn()~ or
    ~null~ if the call to ~fn~ fails.
  * *Choice(fn[])* - Ordered Choice Operator (/): Iterate over ~fn[]~,
    and return the result of the first function that succeeds. It can
    be seen as an OR operation.

  Syntactic Predicate Functions:
  * *Not(fn)* - ~!~: Return true if ~fn~ fails and false if ~fn~
    succeeds;
  * *And(fn)* - ~&~: The opposite of *Not* or ~Not(Not(fn))~;

** Scanner Interface

   The parser that implements the PEG interface listed above is built
   on top of a scanner that implemented all the matching functions and
   the ones that backtrack the input cursor.

   This is the interface that the matching functions depend:
   * *Scanner(input)*: Constructor that creates a new instance of the
     scanner taking the input as a parameter;
   * *Current()*: Return what's under the scanner's cursor;
   * *EOS()*: Determine if the current element is the end of the input;
   * *Error()*: Generate a parsing error;
   * *Expect(e)*: Return the current element under the cursor if it
     matches ~e~ or throw an error otherwise. Doesn't move input
     cursor;
   * *Match(e)*: Return the current element under the cursor if it
     matches ~e~ and advance the cursor by the size of ~e~;
   * *Next()*: Advance the input cursor;

   The parsing function ~Choice~ is also implemented in the scanner
   because it needs direct control over the input cursor in order to
   backtrack before a new option is attempted. E.g.:

   #+begin_src js
   // JavaScript
   function choice(...fns) {
     const saved = cursor; // input cursor
     for (const fn of fns) {
       try { return fn(); }
       catch (e) { cursor = saved; } // backtracking
     }
     throw new Error("None of the options matched");
   }
   #+end_src

   The syntactic predicate ~Not~ is implemented in the scanner as well
   since it also implements backtracking for never consuming the
   input.

   The parser for the PEG grammars was built on top of the scanner
   interface and the PEG functions (ZeroOrMore, Option, Choice, etc).
   The separation of the scanner interface from the implementation of
   the PEG functions allowed the construction of two different
   scanners: one for text and another one for other data structures
   (lists).

** Semantic Actions

   After collecting the results from the matching operations and
   nesting them following the grammar's structure, the PEG library can
   also apply custom functions on the results of each rule
   execution. E.g.:

   #+begin_src js
   // JavaScript
   const semanticActions = {
     D: ({ visit }) => parseInt(visit().join(''), 10),
   };
   const parser = peg.pegc('D <- [0-9]+').bind(semanticActions);
   assertTrue(parser('42') === 42);
   #+end_src

   One of the effects of the infinite look-ahead, and the backtrack
   specifically, is that the entire input has to be consumed before
   deciding if the results are correct or not. This was explored in
   depth in the article [[https://ohmlang.github.io/pubs/dls2016/modular-semantic-actions.pdf][Modular Semantic Actions]] and the general
   suggestion this implementation follows is that the semantic action
   application only happens after parsing finishes successfully.

* The Compiler

  Now that we covered the PEG implementation, we're ready to tackle
  the compilation process itself!

** Parsing the program text

   The first stage of the compiler [[https://github.com/clarete/effigy/blob/master/lang.peg][is a PEG grammar]] that scan and
   parse the program text and generate an [[https://en.wikipedia.org/wiki/Abstract_syntax_tree][Abstract Syntax Tree]] (or AST
   for short) off the syntax I made up.  The semantic actions
   associated with that grammar join lists of characters into words,
   convert lists of digits into numbers, tweak the shape of the AST to
   make it less verbose and easier to be traversed and lastly help
   overcoming two shortcomings of the PEG implementation:

   1. Handle left recursion
   2. Decide if a result should be wrapped into the name of its
      parsing rule

   There are ways to handling left recursion on PEGs. The nicest one I
   found was via **Bounded Left Recursion**.  That approach is
   described in depth in the article [[http://www.inf.puc-rio.br/~roberto/docs/sblp2012.pdf][Left Recursion in Parsing
   Expression Grammars]], but I didn't get to fully implement it, so I
   put it aside to focus on getting to a working compiler.

   The second problem of wrapping captured values with the rule name
   or not could have been fixed by adding a new operator to the PEG
   implementation and resolved at the grammar level.  But instead I
   chose to implement that using semantic actions since the code
   needed was simple although a bit verbose.  But everything else
   worked out pretty smoothly.  That's enough of background, let's
   look at an example. The following code:

   #+begin_src effigy
   fn sum(a, b) a + b
   print(sum(2, 3))
   #+end_src

   should generate the following AST:

   #+begin_src effigy
   ['Module',
     [['Statement',
       ['Function',
        ['sum',
         ['Params', [['Param', 'a'], ['Param', 'b']]],
         ['Code',
          ['Statement',
           ['BinOp', ['Load', 'a'], '+', ['Load', 'b']]]]]]],
      ['Statement',
       ['Call',
        [['Load', 'print'],
         [['Call',
           [['Load', 'sum' ],
            [['Value', ['Number', 2]],
             ['Value', ['Number', 3]]]]]]]]]]]
   #+end_src

   Notice that ~fn sum(a, b) { return a + b }~ outputs the same tree
   as ~fn sum(a, b) a + b~.  Code blocks accept either a single
   statement or a list of statements within curly brackets (~{}~).

** Mapping out scope rules

   After generating the AST during the text parsing phase, we need to
   go through an aditional step before translating the AST into
   /bytecode/.  The scope of every variable needs to be mapped into
   one of the three categories:

   1. Local variables
   2. Global variables
   3. Free variables

   Let's look at the following code snippet to talk about it:

   #+begin_src effigy
   fn plus_n(x) fn(y) x + y
   plus_five = plus_n(5)
   print(plus_five(2)) # Equals 7
   #+end_src

   In the example above, ~x~ is declared at the scope created by the
   ~plus_n~ function and must be available when it's summed to ~y~
   within the scope of the anonymous function.  The variable ~y~ is a
   local variable since it gets created and destroyed within the same
   scope, but ~x~ is a free variable.

   Free variables are variables available in the lexical scope that
   must be kept around to be used when the scope that declared these
   variables isn't around anymore.

   Global variables seem to exist in Python for performance reasons.
   The Python interpreter skips look ups on the local scope for names
   that are known to be available in the module scope or within the
   built-in module, like the name ~print~ in the example above.

   The process of mapping variables into the aforementioned categories
   is done by traversing the AST using a [[https://github.com/clarete/effigy/blob/master/lang.tr][second PEG grammar]] for
   parsing lists instead of a stream of characters.  During that
   process, a symbol table is built and the AST is annotated with
   information that allows the translation phase to look up each
   variable in the symbol table.

   The following Effigy snippet

   #+begin_src effigy
   fn plus_n(x) fn (y) x + y
   #+end_src

   generates an annotated AST that looks like this:

   #+begin_src effigy
   ['Module',
     [['Statement',
       ['Function',
        [['ScopeId', 2], 'plus_n',
         ['Params', [['Param', 'x']]],
         ['Code',
          ['Statement',
           ['Lambda',
            [['ScopeId', 1],
             ['Params', [['Param', 'y']]],
             ['Code',
              ['Statement',
               ['BinOp',
                ['Load', 'x'], '+', ['Load', 'y']]]]]]]]]]]]]
   #+end_src

   The ~ScopeId~ nodes introduced within each scope are used during
   the compilation process to look up the nth entry within the current
   scope of the symbol table.  Here's a simplified view of the list of
   fields a symbol table for the above snippet contains:

   #+begin_src effigy
   [{
     node: 'module',
     fast: [],
     deref: [],
     globals: [],
     children: [{
       node: 'function',
       fast: [],
       deref: ['x'],
       globals: [],
       children: [{
         node: 'lambda',
         fast: ['y'],
         deref: ['x'],
         globals: [],
         children: []
       }]
     }]
   }]
   #+end_src

   One last thing that might be interesting to mention about scopes is
   that Python tries to figure out if a variable is a free variable by
   comparing where it was assigned and where it was used.  If it is
   assigned in the same scope that it's being used, it is a local
   variable.  If it is assigned in an enclosing scope, it is a free
   variable.  If one needs to reassign a free variable in an inner
   scope, the [[https://www.python.org/dev/peps/pep-3104/][nonlocal]] keyword is required to inform the Python
   compiler that the assignment isn't local.

   I chose a slightly different way to allow reassigning free
   variables from enclosing scopes.  Effigy provides the ~let~ keyword
   to mark variables as free variables at the outer scope:

   #+begin_src effigy
   fn f(input) {
     let c = 0
     fn next() {
       value = input[c]
       c = c + 1
       return value
     }
     return next
   }
   cursor = f("word")
   print(cursor()) # will print "w"
   print(cursor()) # will print "o"
   #+end_src

   I haven't used Effigy enough to know if that was a good choice
   though :)

   I bet there might be a way of bundling the symbol table and
   generating the code in a single pass, but that wasn't the route I
   took.  Quite a few decisions I made for handling variable scope
   were inspired by the beautiful write up [[https://codewords.recurse.com/issues/seven/dragon-taming-with-tailbiter-a-bytecode-compiler][Dragon taming with
   Tailbiter, a bytecode compiler for Python]] and that's the route that
   Darius Bacon took on his experiment.  I highly recommend reading
   that post.  It's enlightening and might help understanding the rest
   of this post since I won't get into too many details about how
   Python itself woks.

** Generation of Python programs

   Once the AST is annotated by the scope traversal step, it is ready
   to be fed once again into the [[https://github.com/clarete/effigy/blob/master/lang.tr][second PEG grammar]] and to be
   traversed once more, but now with the intent of driving the
   assembler to generate the final Python program.  In this step, the
   functions (and modules) in Effigy are assembled into [[https://docs.python.org/3.7/library/dis.html#python-bytecode-instructions][/bytecode/
   instructions]] and bundled into [[https://docs.python.org/3/c-api/code.html][Code objects]].

   Instances of Code objects store the /bytecode/ within the ~co_code~
   attribute and metadata, like the number of arguments a function
   receives (~co_argcount~) and the number of local variables
   (~co_nlocals~).  The other very important data Code objects store
   is tables with values.  There's one for literal values
   (~co_consts~), one for local variables (~co_varnames~), one for
   free variables (~co_freevars~) and one for global variables
   (~co_names~).

   All these tables are indexed with integers and carry ~PyObject~
   instances within them.  And since functions themselves are inherit
   from ~PyObject~, Code object is a recursive data type.

   This last traversal of the compiler is also responsible for
   inserting data into the values tables.  Most of the information
   comes from the symbol table built in the previous step.

   When the compiler enters a new scope, a Code object instance is
   created.  When the compiler leaves that scope, the Code object
   instance is returned and bundled within the outer Code object, up
   until the module scope, which is the top one. When that happens, it
   is written as binary data into a file.

** Code Generation

   Generating the assembly code for filling in the ~co_code~
   attributes of Code objects is certainly the biggest task performed
   by the compiler.  Let's take a look at how the compiler would
   generate code for the following expression ~2 + 3 * 4~.

   First the following AST is generated:

   #+begin_src effigy
   ['Module',
    ['Statement',
     ['BinOp',
      ['Value', ['Number', 2]],
      '+',
      ['BinOp',
       ['Value', ['Number', 3]],
       '*',
       ['Value', ['Number', 4]]]]]]
   #+end_src

   When the translation phase takes the above tree as input, it will
   need to do generate the following code:

   #+begin_src effigy
   {
     constants: [2, 3, 4, null],
     instructions: [
       ['load-const', 0],
       ['load-const', 1],
       ['load-const', 2],
       ['binary-multiply'],
       ['binary-add'],
       ['pop-top'],
       ['load-const', 3],
       ['return-value'],
     ]
   }
   #+end_src

  * enter
  * leave
  * emit
  * attr
  * ref
  * pos
  * fix

* Final Thoughts
