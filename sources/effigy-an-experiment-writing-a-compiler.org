#+TITLE: Effigy, an experiment writing a compiler

I've always been fascinated with programming languages.  It's
incredible to me that there are programs that can take the text
representation of other programs and turn them into code that can be
understood by machines.  It's the tool that turn our ideas into
something executable.

All that fun is brought to us by an essential type of software:
compilers!  For quite some time I feared them and thought of them as
sorcery.  Something that mere mortals didn't have access or even the
right to understand.  I'm delighted to say that I was wrong.  Lot's of
people understand how compilers work and the more I dig into it, the
more I realize that it's an extremely human technology.

Wanting to build my own programming languages, I started to write and
rewrite [[https://github.com/clarete/wheelbarrow/blob/master/lispinho/js/main.js][lots]] [[https://github.com/clarete/yal][of]] [[https://gist.github.com/clarete/03e825a70c4b4047468cc9d07ec47e4b][little]] [[https://github.com/clarete/wheelbarrow/blob/master/lispinho/js/main2.js][lisp]] interpreters, which is a common path for
beginners.  I highly recommend you to try that if you're starting your
own compiler writing journey.  Writing all those toy interpreters
taught me a whole lot of new tricks that made me a better software
engineer in general.  It gave me a deeper understanding on what goes
on behind the scenes of a programming language and allowed me to
pursue deeper understanding on each step of the compilation process.

Production ready compilers can have a whole lot of different phases
and components. But today I want to share the design of [[https://github.com/clarete/effigy][Effigy]], a
little toy compiler I wrote that targets the Python Virtual Machine.

#+BEGIN_note
*Note*: Maybe there's some confusion between what are scripting
languages and compiled languages.  I don't need that definition to
talk about my experiment, but Python does have a [[https://github.com/python/cpython/blob/master/Python/compile.c][compiler]] and a
[[https://github.com/python/cpython/blob/master/Python/ceval.c][virtual machine]] that executes the /bytecode/ generated by the
compiler.
#+END_note

* A 38k feet view of the process

  The text representation of the program must be parsed into a tree
  structure that contains the meaning described in the text program.
  That tree will then be traversed and the /bytecode/ will be emitted.

  #+BEGIN_centralized
  [[./effigy-an-experiment-writing-a-compiler-overview.png]]
  #+END_centralized

  I'm not talking about a very important phase between the tree
  traversal and the /bytecode/ generation: The optimization phase.
  Although I'm interested in the subject, this experiment was about
  getting the simplest working compiler up rather making anything
  fast. If performance was a goal, using something like [[http://llvm.org][LLVM]] as the
  backend would be more appropriate.

  The initial transformation phase that converts text into a parse
  tree and the second transformation phase that takes the parse tree
  as input and control the assembler are both based on [[https://bford.info/pub/lang/peg.pdf][Parsing
  Expression Grammars]].  I learned how to implement the tree scanning
  to be used in the second transformation from the article [[http://www.lua.inf.puc-rio.br/publications/mascarenhas11parsing.pdf][Parsing
  Expression Grammars for Structured Data]].  And during the time that I
  was implementing Effigy, I also found the article [[http://www.vpri.org/pdf/tr2010003_PEG.pdf][PEG-based
  transformer provides front-, middle and back-end stages in a simple
  compiler]] that made me feel more comfortable with my choice and made
  me tempted to re-write the assembler with a PEG too, but I never got
  to it.

  Although it's not a requirement to understand this post, I highly
  recommend reading the articles above as they were essential for me
  to really understand how PEGs could be so flexible and powerful.

* The PEG library

  Since two out of the three main components of the compiler were
  built on top of the PEG implementation, the PEG implementation
  itself is a pretty big part of this project which makes it worth
  talking about it with some depth before moving on to the actual
  compiler.

  The main goal of the PEG library is to parse input based on a
  user-defined grammar. That happens in roughly three steps:

  1. The user-defined grammar is parsed into a grammar tree;
  2. The grammar tree and the input are fed to an interpreter that
     will traverse the grammar trying to match the input and generate
     a parse tree from a successful match;
  3. The parse tree will be traversed and semantic actions will be
     applied;
     
  The first step implements a [[https://en.wikipedia.org/wiki/Recursive_descent_parser][recursive descent parser]] that is able to
  understand the syntax of PEG grammars and create a tree that will be
  traversed by the second step. E.g.: Parsing the grammar

  #+begin_src peg
  D <- [0-1]+
  #+end_src

  should yield the following grammar tree:

  #+begin_src js
  {
    'D': [[
      [{ name: 'oneOrMore' },
        [{ name: 'range' }, '0', '1']]
    ]]
  }
  #+end_src

  In that format, lists are considered lists unless their first item
  is an object rather than a string. In that case, they're seen as
  functions and the rest of the elements in the list are considered
  parameters. That might resemble Lisp's [[https://en.wikipedia.org/wiki/S-expression][S-Expressions]] for some and
  this format is the simplest I found to make the grammar interpreter
  as simple as possible as well.

  The first rule to appear in the grammar becomes the starting
  rule. That could easily be more customizable, but again I was going
  for simplicity. In the example above, when that grammar tree is
  traversed the primitive functions are executed in a top-down
  fashion.  Which means that the function ~oneOrMore~ will receive a
  [[https://en.wikipedia.org/wiki/Thunk][thunk]] of the call to ~range~ with its parameters and execute it
  until it fails.

  When the starting rule is passed to the interpreter, it will either
  execute a matching function directly or defer it to another rule.
  The set of matching and parsing functions available are the ones
  documented in Ford's paper with the extension for parsing lists from
  Medeiros' paper.

  Matching Functions:
  * *Any()* - ~.~: Matches any character but fails on ~EOF~;
  * *Literal(c)* - ~"c"~: Fail if ~c~ doesn't match the character
    under the input cursor;
  * *Class(c[])* - ~[abcd]~: Fail if none of the elements of ~c[]~
    match the character under the input cursor;
  * *Range(ca, cb)* ~[a-z]~: Fail if the current character under the
    input cursor isn't between the characteres ~ca~ and ~cb~;

  Parsing Functions:
  * *ZeroOrMore(fn)* - Star Operator (*): Execute ~fn~ indefinitely
    until it fails.  All collected results are returned. It never
    fails even if it fails in the first call;
  * *OneOrMore(fn)* - Plus Operator (+): Execute ~fn~ once failing if
    this first call fails. If the first call succeeds, then prepend
    this result to the output of ~ZeroOrMore(fn)~
  * *Option(fn)* - Option Operator (?): Return the result of ~fn()~ or
    ~null~ if the call to ~fn~ fails.
  * *Choice(fn[])* - Ordered Choice Operator (/): Iterate over ~fn[]~,
    and return the result of the first function that succeeds. It can
    be seen as an OR operation.

  Syntactic Predicate Functions:
  * *Not(fn)* - ~!~: Return true if ~fn~ fails and false if ~fn~
    succeeds;
  * *And(fn)* - ~&~: The opposite of *Not* or ~Not(Not(fn))~;

** Scanner Interface

   Bootstrapping the PEG implementation took the implementation of a
   recursive parser for grammars built on top of a scanner that
   implemented all the matching functions and control backtracking.

   This is the interface that the matching functions depend:
   * *Scanner(input)*: Constructor that creates a new instance of the
     scanner taking the input as a parameter;
   * *Current()*: Return what's under the scanner's cursor;
   * *EOS()*: Determine if the current element is the end of the input;
   * *Error()*: Generate a parsing error;
   * *Expect(e)*: Return the current element under the cursor if it
     matches ~e~ or throw an error otherwise. Doesn't move input
     cursor;
   * *Match(e)*: Return the current element under the cursor if it
     matches ~e~ and advance the cursor by the size of ~e~;
   * *Next()*: Advance the input cursor;

   The parsing function ~Choice~ is also implemented in the scanner
   because it needs direct control over the input cursor in order to
   backtrack before a new option is attempted. E.g.:

   #+begin_src js
     function choice(...fns) {
       const saved = cursor; // input cursor
       for (const fn of fns) {
         try { return fn(); }
         catch (e) { cursor = saved; } // backtracking
       }
       throw new Error("None of the options matched");
     }
   #+end_src

   The syntactic predicate ~Not~ is implemented in the scanner as well
   since it also implements backtracking.

   With the this scanner interface available, it was possible to write
   a parser for the PEG grammar.  The separation of the scanner
   interface from the implementation of the PEG interpreter allowed
   the construction of two different scanners: one for text and
   another one for other data structures (lists).

** Semantic Actions

   After collecting the results the parsing rules and nesting them
   following the grammar's structure, this PEG library also provides a
   semantic action mechanism that applies custom functions on the
   results of each rule execution. E.g.:

   #+begin_src js
   const semanticActions = {
     D: ({ visit }) => parseInt(visit().join(''), 10),
   };
   const parser = peg.pegc('D <- [0-9]+').bind(semanticActions);
   assertTrue(parser('42') === 42);
   #+end_src

   One of the effects of the infinite look-ahead, and the backtrack
   specifically, is that the entire input has to be consumed before
   deciding if the results are correct or not. This was explored in
   depth in the article [[https://ohmlang.github.io/pubs/dls2016/modular-semantic-actions.pdf][Modular Semantic Actions]] and the general
   suggestion this implementation followed is that the semantic action
   traversal will happen after the parsing finishes successfully.

* The Compiler

  Now that we covered how the PEG implementation works, we're ready to
  tackle the compilation process itself!

** Parsing the program text

   The first stage of the compiler [[https://github.com/clarete/effigy/blob/master/lang.peg][is a PEG grammar]] that scan and
   parse the program text and generate an [[https://en.wikipedia.org/wiki/Abstract_syntax_tree][Abstract Syntax Tree]] (or AST
   for short) off the syntax I made up.  The semantic actions
   associated with that grammar will join lists of characters into
   words, convert lists of digits into numbers, make the shape of the
   AST less verbose and easier to be traversed and lastly help
   overcoming two shortcomings of the PEG implementation:

   1. Handle left recursion
   2. Decide if a result should be wrapped into the name of its
      parsing rule

   There are ways to handling left recursion on PEGs. The nicest one I
   found was via **Bounded Left Recursion**.  That approach is
   described in depth in the article [[http://www.inf.puc-rio.br/~roberto/docs/sblp2012.pdf][Left Recursion in Parsing
   Expression Grammars]], but I didn't get to fully implement it, so I
   put it aside to focus on getting to a working compiler.

   The second problem of wrapping captured values with the rule name
   or not could have been fixed by adding a new operator to the PEG
   implementation and resolved at the grammar level. But I chose to
   just implement that using the semantic actions since the code
   needed was simple although a bit verbose.  But everything else
   worked out pretty smoothly. Let's look at an example.

   The following code:

   #+begin_src effigy
   fn sum(a, b) a + b
   print(sum(2, 3))
   #+end_src

   should generate the following AST:

   #+begin_src effigy
   ['Module',
     [['Statement',
       ['Function',
        ['sum',
         ['Params', [['Param', 'a'], ['Param', 'b']]],
         ['Code',
          ['Statement',
           ['BinOp', ['Load', 'a'], '+', ['Load', 'b']]]]]]],
      ['Statement',
       ['Call',
        [['Load', 'print'],
         [['Call',
           [['Load', 'sum' ],
            [['Value', ['Number', 2]],
             ['Value', ['Number', 3]]]]]]]]]]]
   #+end_src

   Notice that ~fn sum(a, b) { return a + b }~ outputs the same tree
   as ~fn sum(a, b) a + b~.  Code blocks accept either a single
   statement or a list of statements within curly brackets (~{}~).

** Mapping out scope rules

   After generating the AST during the text parsing phase, we need to
   go through an aditional step before translating the tree into
   /bytecode/.  The scope of every variable needs to be mapped into
   three categories:

   1. Local variables
   2. Global variables
   3. Free variables

   Let's look at the following code snippet to talk about it:

   #+begin_src effigy
   fn plus_n(x) fn(y) x + y
   plus_five = plus_n(5)
   print(plus_five(2)) # Equals 7
   #+end_src

   In the example above, ~x~ is declared at the scope created by the
   ~plus_n~ function and must be available when it's summed to ~y~
   within the scope of the anonymous function.  The variable ~y~ is a
   local variable since it gets created and destroyed within the same
   scope, but ~x~ is a free variable.

   Free variables are variables available in the lexical scope that
   must be kept around to be used when the scope that declared these
   variables isn't around anymore.

   Global variables seem to exist in Python for performance reasons
   only.  The Python interpreter will skip looking up the local scope
   for names that are only available in the module scope or within the
   built-in module, like the name ~print~ in the example above.

   The process of mapping variables into the aforementioned categories
   is done by traversing the AST using a [[https://github.com/clarete/effigy/blob/master/lang.tr][second PEG grammar]] for
   parsing lists instead of characters.  During that process, a symbol
   table is built and the AST is annotated with information that will
   allow the translation phase to look up each variable in the symbol
   table.

   The following effigy code snippet

   #+begin_src effigy
   fn plus_n(x) fn (y) x + y
   #+end_src

   will generate an annotated AST that looks like this:

   #+begin_src effigy
   ['Module',
     [['Statement',
       ['Function',
        [['ScopeId', 2], 'plus_n',
         ['Params', [['Param', 'x']]],
         ['Code',
          ['Statement',
           ['Lambda',
            [['ScopeId', 1],
             ['Params', [['Param', 'y']]],
             ['Code',
              ['Statement',
               ['BinOp',
                ['Load', 'x'], '+', ['Load', 'y']]]]]]]]]]]]]
   #+end_src

   The ~ScopeId~ nodes introduced within each scope are used during
   the compilation process to look up the nth entry within the current
   scope of the symbol table.  Here's a simplified view of the list of
   fields a symbol table contains:

   #+begin_src effigy
   [{
     fast: [],     # Local variables
     deref: [],    # Variables from enclosing scope
     globals: [],  # Globals/Built-ins
     children: [{
       fast: [], deref: [], globals: [], children: []
     }]
   }]
   #+end_src

   One last thing that might be interesting to mention about scopes is
   that Python tries to figure out if a variable is a free variable by
   comparing where it was assigned and where it was used.  If it was
   assigned in the same scope that it's being used, it is a local
   variable.  If it was assigned in an enclosing scope, it is a free
   variable.  That, if one needs to reassign a free variable, the
   [[https://www.python.org/dev/peps/pep-3104/][nonlocal]] keyword is required to inform the Python compiler that a
   it the assignment isn't local.

   I chose a slightly different way to allow reassigning free
   variables from enclosing scopes.  Effigy uses the ~let~ keyword to
   mark variables as free variables at the outer scope:

   #+begin_src effigy
   fn f(input) {
     let c = 0
     fn next() {
       value = input[c]
       c = c + 1
       return value
     }
     return next
   }
   cursor = f("word")
   print(cursor()) # will print "w"
   print(cursor()) # will print "o"
   #+end_src

   I haven't used Effigy enough to know if that was a good choice
   though :)

** Translation (Assembly)

   Once the parse tree is left in a good enough shape by the syntax
   stage, it is ready to be fed into [[https://github.com/clarete/effigy/blob/master/lang.tr][the second PEG grammar]] that will
   drive the compiler to generate the Python program.  This is where
   things become less about any compiler and more about how itself
   Python works.

   The translation from the parse tree to /bytecode/ was done in a two
   step process:

   1. Build the symbol table.  Before the compiler can emit any code,
      it must first determine the scope of each variable. In Python
      they can be either local (declared within the function), global
      (defined in the main scope of a module) or bound to the lexical
      scope (closures).

   2. Traverse the annotated tree and use the assembler to emit the
      actual /bytecode/.  This step depends on the annotations left by
      the syntax stage to determine how to write and read from
      variables.

   Building the symbol table 

   The assembler exposes a simple interface

  * enter
  * leave
  * emit
  * attr
  * ref
  * pos
  * fix

* Final Thoughts
