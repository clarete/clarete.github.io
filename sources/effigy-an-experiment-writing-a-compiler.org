#+TITLE: Effigy, an experiment writing a compiler

I've always been fascinated with programming languages.  It's
incredible to me that there are programs that can take the text
representation of other programs and turn them into code that can be
understood by machines.  It's the tool that turn our ideas into
something executable.

All that fun is brought to us by an essential type of software:
compilers!  For quite some time I feared them and thought of them as
sorcery.  Something that mere mortals didn't have access or even the
right to understand.  I'm delighted to say that I was wrong.  Lot's of
people understand how compilers work and the more I dig into it, the
more I realize that it's an extremely human technology.

Wanting to build my own programming languages, I started to write and
rewrite [[https://github.com/clarete/wheelbarrow/blob/master/lispinho/js/main.js][lots]] [[https://github.com/clarete/yal][of]] [[https://gist.github.com/clarete/03e825a70c4b4047468cc9d07ec47e4b][little]] [[https://github.com/clarete/wheelbarrow/blob/master/lispinho/js/main2.js][lisp]] interpreters, which is a common path for
beginners.  I highly recommend you to try that if you're starting your
own compiler writing journey.  Writing all those toy interpreters
taught me a whole lot of new tricks that made me a better software
engineer in general.  It gave me a deeper understanding on what goes
on behind the scenes of a programming language and allowed me to
pursue deeper understanding on each step of the compilation process.

Production ready compilers can have a whole lot of different phases
and components. But today I want to share the design of [[https://github.com/clarete/effigy][Effigy]], a
little toy compiler I wrote that target the Python Virtual Machine.

#+BEGIN_note
*Note*: Maybe there's some confusion between what are scripting
languages and compiled languages.  I don't need that definition to
talk about my experiment, but Python does have a [[https://github.com/python/cpython/blob/master/Python/compile.c][compiler]] and a
[[https://github.com/python/cpython/blob/master/Python/ceval.c][virtual machine]] that executes the /bytecode/ generated by the
compiler.
#+END_note

* A 38k feet view of the process

  The text representation of the program must be parsed into a tree
  structure that contains the meaning described in the text program.
  That tree will then be traversed and the /bytecode/ will be emitted.

  #+BEGIN_centralized
  [[./overview.png]]
  #+END_centralized

  I'm not talking about a very important phase between the tree
  traversal and the /bytecode/ generation: The optimization phase.
  Although I'm interested in the subject, this experiment was about
  getting the simplest working compiler up rather making anything
  fast. If performance was a goal, using something like [[http://llvm.org][LLVM]] as the
  backend would be more appropriate.

  The initial transformation phase that converts text into a parse
  tree and the second transformation phase that takes the parse tree
  as input and control the assembler are both based on [[https://bford.info/pub/lang/peg.pdf][Parsing
  Expression Grammars]].  I learned how to implement the tree scanning
  to be used in the second transformation from the article [[http://www.lua.inf.puc-rio.br/publications/mascarenhas11parsing.pdf][Parsing
  Expression Grammars for Structured Data]].  And during the time that I
  was implementing Effigy, I also found the article [[http://www.vpri.org/pdf/tr2010003_PEG.pdf][PEG-based
  transformer provides front-, middle and back-end stages in a simple
  compiler]] that made me feel more comfortable with my choice and made
  me tempted to re-write the assembler with a PEG too, but I never got
  to it.

  Although it's not a requirement to understand this post, I highly
  recommend reading the articles above as they were essential for me
  to really understand how PEGs could be so flexible and powerful.

* The PEG library

  Since two out of the three main components of the compiler were
  built on top of the PEG implementation, the PEG implementation
  itself is a pretty big part of this project which makes it worth
  talking about it with some depth before moving on to the actual
  compiler.

  The main goal of the PEG library is to parse input based on a
  user-defined grammar. That happens in roughly three steps:

  1. The user-defined grammar is parsed into a grammar tree;
  2. The grammar tree and the input are fed to an interpreter that
     will traverse the grammar trying to match the input and generate
     a parse tree from a successful match;
  3. The parse tree will be traversed and semantic actions will be
     applied;
     
  The first step implements a [[https://en.wikipedia.org/wiki/Recursive_descent_parser][recursive descent parser]] that is able to
  understand the syntax of PEG grammars and create a tree that will be
  traversed by the second step. E.g.: Parsing the grammar

  #+begin_src peg
  D <- [0-1]+
  #+end_src

  should yield the following grammar tree:

  #+begin_src js
  {
    'D': [[
      [{ name: 'oneOrMore' },
        [{ name: 'range' }, '0', '1']]
    ]]
  }
  #+end_src

  In that format, lists are considered lists unless their first item
  is an object rather than a string. In that case, they're seen as
  functions and the rest of the elements in the list are considered
  parameters. That might resemble Lisp's [[https://en.wikipedia.org/wiki/S-expression][S-Expressions]] for some and
  this format is the simplest I found to make the grammar interpreter
  as simple as possible as well.

  The first rule to appear in the grammar becomes the starting
  rule. That could easily be more customizable, but again I was going
  for simplicity. In the example above, when that grammar tree is
  traversed the primitive functions are executed in a top-down
  fashion.  Which means that the function ~oneOrMore~ will receive a
  [[https://en.wikipedia.org/wiki/Thunk][thunk]] of the call to ~range~ with its parameters and execute it
  until it fails.

  When the starting rule is passed to the interpreter, it will either
  execute a matching function directly or defer it to another rule.
  The set of matching and parsing functions available are the ones
  documented in Ford's paper with the extension for parsing lists from
  Medeiros' paper.

  Matching Functions:
  * *Any()* - ~.~: Matches any character but fails on ~EOF~;
  * *Literal(c)* - ~"c"~: Fail if ~c~ doesn't match the character
    under the input cursor;
  * *Class(c[])* - ~[abcd]~: Fail if none of the elements of ~c[]~
    match the character under the input cursor;
  * *Range(ca, cb)* ~[a-z]~: Fail if the current character under the
    input cursor isn't between the characteres ~ca~ and ~cb~;

  Parsing Functions:
  * *ZeroOrMore(fn)* - Star Operator (*): Execute ~fn~ indefinitely
    until it fails.  All collected results are returned. It never
    fails even if it fails in the first call;
  * *OneOrMore(fn)* - Plus Operator (+): Execute ~fn~ once failing if
    this first call fails. If the first call succeeds, then prepend
    this result to the output of ~ZeroOrMore(fn)~
  * *Option(fn)* - Option Operator (?): Return the result of ~fn()~ or
    ~null~ if the call to ~fn~ fails.
  * *Choice(fn[])* - Ordered Choice Operator (/): Iterate over ~fn[]~,
    and return the result of the first function that succeeds. It can
    be seen as an OR operation.

  Syntactic Predicate Functions:
  * *Not(fn)* - ~!~: Return true if ~fn~ fails and false if ~fn~
    succeeds;
  * *And(fn)* - ~&~: The opposite of *Not* or ~Not(Not(fn))~;

** Scanner Interface

   Bootstrapping the PEG implementation took the implementation of a
   recursive parser for grammars built on top of a scanner that
   implemented all the matching functions and control backtracking.

   This is the interface that the matching functions depend:
   * *Scanner(input)*: Constructor that creates a new instance of the
     scanner taking the input as a parameter;
   * *Current()*: Return what's under the scanner's cursor;
   * *EOS()*: Determine if the current element is the end of the input;
   * *Error()*: Generate a parsing error;
   * *Expect(e)*: Return the current element under the cursor if it
     matches ~e~ or throw an error otherwise. Doesn't move input
     cursor;
   * *Match(e)*: Return the current element under the cursor if it
     matches ~e~ and advance the cursor by the size of ~e~;
   * *Next()*: Advance the input cursor;

   The parsing function ~Choice~ is also implemented in the scanner
   because it needs direct control over the input cursor in order to
   backtrack before a new option is attempted. E.g.:

   #+begin_src js
     function choice(...fns) {
       const saved = cursor; // input cursor
       for (const fn of fns) {
         try { return fn(); }
         catch (e) { cursor = saved; } // backtracking
       }
       throw new Error("None of the options matched");
     }
   #+end_src

   The syntactic predicate ~Not~ is implemented in the scanner as well
   since it also implements backtracking.

   With the this scanner interface available, it was possible to write
   a parser for the PEG grammar.  The separation of the scanner
   interface from the implementation of the PEG interpreter allowed
   the construction of two different scanners: one for text and
   another one for other data structures (lists).

** Semantic Actions

   After collecting the results the parsing rules and nesting them
   following the grammar's structure, this PEG library also provides a
   semantic action mechanism that applies custom functions on the
   results of each rule execution. E.g.:

   #+begin_src js
   const semanticActions = {
     D: ({ visit }) => parseInt(visit().join(''), 10),
   };
   const parser = peg.pegc('D <- [0-9]+').bind(semanticActions);
   assertTrue(parser('42') === 42);
   #+end_src

   One of the effects of the infinite look-ahead, and the backtrack
   specifically, is that the entire input has to be consumed before
   deciding if the results are correct or not. This was explored in
   depth in the article [[https://ohmlang.github.io/pubs/dls2016/modular-semantic-actions.pdf][Modular Semantic Actions]] and the general
   suggestion this implementation followed is that the semantic action
   traversal will happen after the parsing finishes successfully.

* The Compiler

  Now that we covered how the PEG implementation works, we're ready to
  tackle the compilation process itself!

** Syntax

   The first stage of the compiler [[https://github.com/clarete/effigy/blob/master/lang.peg][is a PEG grammar]] that generates the
   parse tree off the syntax I made up.  The semantic actions
   associated with that grammar serve mainly the purpose of overcoming
   two shortcomings of the PEG implementation:

   1. Handling left recursion
   2. Deciding if a result should be wrapped into the name of the rule
      or not. This is mostly important to prepare the parse tree to be
      traversed by the translation grammar.

   I did find a great mental model for handling left recursion on
   PEGs. It's called **Bounded Left Recursion** and is described in
   depth in the article [[http://www.inf.puc-rio.br/~roberto/docs/sblp2012.pdf][Left Recursion in Parsing Expression Grammars]],
   but I didn't get to fully implement it, so I put it aside to focus
   on getting to a working compiler.

   The second problem of making a decision about wrapping results with
   the rule name could have been fixed by adding a new operator to the
   grammar language to allow the user defining the gramar to decide
   which rules should be marked with the rule name, but I chose to
   keep the complexity on the PEG side manageable and implement that
   in the compiler since the code I needed to achieve that was simple
   although a bit verbose.

   If you're curious to know how the syntax looks like, it pretty much
   is the Python syntax replacing the indentation requirements with
   brackets! I know it might not be exactly humor Python or JavaScript
   developers but maybe that was my own inside joke. :)

   Here's what fibonacci looks like

   #+begin_src effigy
   fn fib(n) {
     if n < 2 return n
     a = 1; b = 1; f = 0; count = 0
     while count < n - 2 {
       f = a + b; a = b; b = f
       count = count + 1
     }
     return f
   }
   #+end_src

** Translation (Assembly)

   Once the parse tree is left in a good enough shape by the syntax
   stage, it is ready to be fed into [[https://github.com/clarete/effigy/blob/master/lang.tr][the second PEG grammar]] that will
   drive the compiler to generate the Python program.  This is where
   things become less about any compiler and more about how itself
   Python works.

   The translation from the parse tree to /bytecode/ was done in a two
   step process:

   1. Build the symbol table.  Before the compiler can emit any code,
      it must first determine the scope of each variable. In Python
      they can be either local (declared within the function), global
      (defined in the main scope of a module) or bound to the lexical
      scope (closures).

   2. Traverse the annotated tree and use the assembler to emit the
      actual /bytecode/.  This step depends on the annotations left by
      the syntax stage to determine how to write and read from
      variables.

   Building the symbol table 

   The assembler exposes a simple interface

  * enter
  * leave
  * emit
  * attr
  * ref
  * pos
  * fix

* Final Thoughts
